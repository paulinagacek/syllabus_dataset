{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053b07b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Position Rank\n",
      "Precision: 0.53\n",
      "Recall: 0.85\n",
      "F1 Score: 0.64\n",
      "Precision@O: 0.56\n",
      "Recall@O: 0.56\n",
      "F1@O Score: 0.56\n",
      "\n",
      "\n",
      "### YAKE\n",
      "Precision: 0.39\n",
      "Recall: 0.76\n",
      "F1 Score: 0.51\n",
      "Precision@O: 0.56\n",
      "Recall@O: 0.56\n",
      "F1@O Score: 0.56\n",
      "\n",
      "\n",
      "### Key BERT\n",
      "Precision: 0.33\n",
      "Recall: 0.65\n",
      "F1 Score: 0.44\n",
      "Precision@O: 0.46\n",
      "Recall@O: 0.46\n",
      "F1@O Score: 0.46\n",
      "\n",
      "\n",
      "### RAKE\n",
      "Precision: 0.47\n",
      "Recall: 0.74\n",
      "F1 Score: 0.56\n",
      "Precision@O: 0.50\n",
      "Recall@O: 0.50\n",
      "F1@O Score: 0.50\n",
      "\n",
      "\n",
      "### TextRank\n",
      "Precision: 0.53\n",
      "Recall: 0.83\n",
      "F1 Score: 0.63\n",
      "Precision@O: 0.57\n",
      "Recall@O: 0.57\n",
      "F1@O Score: 0.57\n",
      "\n",
      "\n",
      "### LLM Powered Approach\n",
      "Precision: 0.78\n",
      "Recall: 0.78\n",
      "F1 Score: 0.77\n",
      "Precision@O: 0.82\n",
      "Recall@O: 0.77\n",
      "F1@O Score: 0.79\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from extractors.extractors import extractors\n",
    "from evaluation.evaluation import evaluate_concepts, read_json_from_file\n",
    "\n",
    "json_data = read_json_from_file(\"NER_prerequisites.json\")\n",
    "f1_results = []\n",
    "f1_at_O_results = []\n",
    "for name, extractor in extractors.items():\n",
    "    evaluation_data = []\n",
    "    for item in json_data:\n",
    "        prerequisite_text = item.get('prerequisite_text', '')\n",
    "        gt_concepts = item.get('concepts', [])\n",
    "        extracted_concepts = extractor.extractKeyWords(prerequisite_text, int(len(gt_concepts)*2))\n",
    "        evaluation_data.append((gt_concepts, extracted_concepts))\n",
    "    results = evaluate_concepts(name, evaluation_data)\n",
    "    f1_results.append(results['f1'])\n",
    "    f1_at_O_results.append(results['f1_at_O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3cc2b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Paired t-test results vs. LLM (F1) ===\n",
      "Method 1: t = 4.1884, p = 0.0001 -> Significant\n",
      "Method 2: t = 7.7896, p = 0.0000 -> Significant\n",
      "Method 3: t = 8.9242, p = 0.0000 -> Significant\n",
      "Method 4: t = 5.5223, p = 0.0000 -> Significant\n",
      "Method 5: t = 4.5800, p = 0.0000 -> Significant\n",
      "\n",
      "=== Paired t-test results vs. LLM (F1@O) ===\n",
      "Method 1: t = 5.4932, p = 0.0000 -> Significant\n",
      "Method 2: t = 5.3710, p = 0.0000 -> Significant\n",
      "Method 3: t = 7.5544, p = 0.0000 -> Significant\n",
      "Method 4: t = 6.7479, p = 0.0000 -> Significant\n",
      "Method 5: t = 5.2580, p = 0.0000 -> Significant\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Assume last row is your LLM-based method\n",
    "llm_f1 = np.array(f1_results[-1])\n",
    "llm_f1_at_o = np.array(f1_at_O_results[-1])\n",
    "\n",
    "print(\"=== Paired t-test results vs. LLM (F1) ===\")\n",
    "for i, method_f1 in enumerate(f1_results[:-1]):  # Skip LLM row\n",
    "    method_f1 = np.array(method_f1)\n",
    "    t_stat, p_value = ttest_rel(llm_f1, method_f1)\n",
    "    print(f\"Method {i+1}: t = {t_stat:.4f}, p = {p_value:.4f} -> {'Significant' if p_value < 0.05 else 'Not significant'}\")\n",
    "\n",
    "print(\"\\n=== Paired t-test results vs. LLM (F1@O) ===\")\n",
    "for i, method_f1o in enumerate(f1_at_O_results[:-1]):\n",
    "    method_f1o = np.array(method_f1o)\n",
    "    t_stat, p_value = ttest_rel(llm_f1_at_o, method_f1o)\n",
    "    print(f\"Method {i+1}: t = {t_stat:.4f}, p = {p_value:.4f} -> {'Significant' if p_value < 0.05 else 'Not significant'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7324f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wilcoxon signed-rank test vs. LLM (F1) ===\n",
      "Method 1: W = 442.0, p = 0.0000 -> Significant\n",
      "Method 2: W = 206.5, p = 0.0000 -> Significant\n",
      "Method 3: W = 155.0, p = 0.0000 -> Significant\n",
      "Method 4: W = 361.0, p = 0.0000 -> Significant\n",
      "Method 5: W = 380.0, p = 0.0000 -> Significant\n",
      "\n",
      "=== Wilcoxon signed-rank test vs. LLM (F1@O) ===\n",
      "Method 1: W = 224.0, p = 0.0000 -> Significant\n",
      "Method 2: W = 178.0, p = 0.0000 -> Significant\n",
      "Method 3: W = 161.5, p = 0.0000 -> Significant\n",
      "Method 4: W = 156.5, p = 0.0000 -> Significant\n",
      "Method 5: W = 228.5, p = 0.0000 -> Significant\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "import numpy as np\n",
    "\n",
    "# Your LLM-based results\n",
    "llm_f1 = np.array(f1_results[-1])\n",
    "llm_f1_at_o = np.array(f1_at_O_results[-1])\n",
    "\n",
    "print(\"=== Wilcoxon signed-rank test vs. LLM (F1) ===\")\n",
    "for i, method_f1 in enumerate(f1_results[:-1]):  # All baselines\n",
    "    method_f1 = np.array(method_f1)\n",
    "    stat, p = wilcoxon(llm_f1, method_f1)\n",
    "    print(f\"Method {i+1}: W = {stat}, p = {p:.4f} -> {'Significant' if p < 0.05 else 'Not significant'}\")\n",
    "\n",
    "print(\"\\n=== Wilcoxon signed-rank test vs. LLM (F1@O) ===\")\n",
    "for i, method_f1o in enumerate(f1_at_O_results[:-1]):\n",
    "    method_f1o = np.array(method_f1o)\n",
    "    stat, p = wilcoxon(llm_f1_at_o, method_f1o)\n",
    "    print(f\"Method {i+1}: W = {stat}, p = {p:.4f} -> {'Significant' if p < 0.05 else 'Not significant'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
